{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)  # Remove @mentions\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", \"\", text)  # Remove special characters\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        return text\n",
    "    return text  # Return as-is if not a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                @VirginAmerica What @dhepburn said.   \n",
      "1  @VirginAmerica plus you've added commercials t...   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3  @VirginAmerica it's really aggressive to blast...   \n",
      "4  @VirginAmerica and it's a really big bad thing...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                                         what said.  \n",
      "1  plus you've added commercials to the experienc...  \n",
      "2  i didn't today... must mean i need to take ano...  \n",
      "3  it's really aggressive to blast obnoxious ente...  \n",
      "4           and it's a really big bad thing about it  \n"
     ]
    }
   ],
   "source": [
    "print(df[[\"text\", \"cleaned_text\"]].head())\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0                                         what said.   \n",
      "1  plus you've added commercials to the experienc...   \n",
      "2  i didn't today... must mean i need to take ano...   \n",
      "3  it's really aggressive to blast obnoxious ente...   \n",
      "4           and it's a really big bad thing about it   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0                                    [what, said, .]  \n",
      "1  [plus, you, 've, added, commercials, to, the, ...  \n",
      "2  [i, did, n't, today, ..., must, mean, i, need,...  \n",
      "3  [it, 's, really, aggressive, to, blast, obnoxi...  \n",
      "4  [and, it, 's, a, really, big, bad, thing, abou...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "df[\"tokenized_text\"] = df[\"cleaned_text\"].apply(word_tokenize)\n",
    "\n",
    "print(df[[\"cleaned_text\", \"tokenized_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0                                         what said.   \n",
      "1  plus you've added commercials to the experienc...   \n",
      "2  i didn't today... must mean i need to take ano...   \n",
      "3  it's really aggressive to blast obnoxious ente...   \n",
      "4           and it's a really big bad thing about it   \n",
      "\n",
      "                                       filtered_text  \n",
      "0                                             said .  \n",
      "1  plus 've added commercials experience ... tacky .  \n",
      "2   n't today ... must mean need take another trip !  \n",
      "3  's really aggressive blast obnoxious entertain...  \n",
      "4                            's really big bad thing  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)  # Tokenization\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]  # Stopword removal\n",
    "        return \" \".join(filtered_words)  # Join back into a string\n",
    "    return text\n",
    "\n",
    "df[\"filtered_text\"] = df[\"cleaned_text\"].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "print(df[[\"cleaned_text\", \"filtered_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       filtered_text  \\\n",
      "0                                             said .   \n",
      "1  plus 've added commercials experience ... tacky .   \n",
      "2   n't today ... must mean need take another trip !   \n",
      "3  's really aggressive blast obnoxious entertain...   \n",
      "4                            's really big bad thing   \n",
      "\n",
      "                                        stemmed_text  \n",
      "0                                             said .  \n",
      "1             plu 've ad commerci experi ... tacki .  \n",
      "2     n't today ... must mean need take anoth trip !  \n",
      "3  's realli aggress blast obnoxi entertain guest...  \n",
      "4                            's realli big bad thing  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def apply_stemming(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]  # Apply stemming\n",
    "        return \" \".join(stemmed_words)  # Join words back\n",
    "    return text\n",
    "\n",
    "df[\"stemmed_text\"] = df[\"filtered_text\"].apply(apply_stemming)\n",
    "\n",
    "print(df[[\"filtered_text\", \"stemmed_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00  000  000ft  000lb  0011  0016  0162389030167  0162424965446  \\\n",
      "0  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "1  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "2  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "3  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "4  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "\n",
      "   0162431184663  0167560070877  ...  zigzag  zip  zipper  zombi  zone  zoom  \\\n",
      "0            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "1            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "2            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "3            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "4            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "\n",
      "   zrh  zuke  zurich  zurichnew  \n",
      "0  0.0   0.0     0.0        0.0  \n",
      "1  0.0   0.0     0.0        0.0  \n",
      "2  0.0   0.0     0.0        0.0  \n",
      "3  0.0   0.0     0.0        0.0  \n",
      "4  0.0   0.0     0.0        0.0  \n",
      "\n",
      "[5 rows x 11242 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(df[\"stemmed_text\"])  # Apply on stemmed text\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"stemmed_text\"])  # Apply on stemmed text\n",
    "\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 11712 samples\n",
      "Testing Set: 2928 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target labels (y)\n",
    "X = df[\"stemmed_text\"]  # Using stemmed text as input\n",
    "y = df[\"airline_sentiment\"]  # Sentiment labels (positive, negative, neutral)\n",
    "\n",
    "# Split the dataset (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show dataset sizes\n",
    "print(f\"Training Set: {len(X_train)} samples\")\n",
    "print(f\"Testing Set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.99      0.81      1889\n",
      "     neutral       0.72      0.14      0.24       580\n",
      "    positive       0.90      0.19      0.32       459\n",
      "\n",
      "    accuracy                           0.70      2928\n",
      "   macro avg       0.77      0.44      0.46      2928\n",
      "weighted avg       0.73      0.70      0.62      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Convert text into TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df[\"stemmed_text\"])\n",
    "\n",
    "# Define target labels\n",
    "y = df[\"airline_sentiment\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Naïve Bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
