{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load dataset in Vscode\n",
    "2. clean data using regex\n",
    "3. word tokenization\n",
    "4. stopwords removation\n",
    "5. steming\n",
    "6. count vectorization / tf-IDF -> pickle file\n",
    "7. train test split\n",
    "8. create ml model -> pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                @VirginAmerica What @dhepburn said.   \n",
      "1  @VirginAmerica plus you've added commercials t...   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3  @VirginAmerica it's really aggressive to blast...   \n",
      "4  @VirginAmerica and it's a really big bad thing...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                                         what said.  \n",
      "1  plus you've added commercials to the experienc...  \n",
      "2  i didn't today... must mean i need to take ano...  \n",
      "3  it's really aggressive to blast obnoxious ente...  \n",
      "4           and it's a really big bad thing about it  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)  # Remove @mentions\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", \"\", text)  # Remove special characters\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        return text\n",
    "    return text  # Return as-is if not a string\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Tweets.csv\")  # Replace with your file\n",
    "\n",
    "# Apply cleaning function to a text column (e.g., 'text')\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Show cleaned data\n",
    "print(df[[\"text\", \"cleaned_text\"]].head())\n",
    "\n",
    "# Save cleaned dataset (optional)\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0                                         what said.   \n",
      "1  plus you've added commercials to the experienc...   \n",
      "2  i didn't today... must mean i need to take ano...   \n",
      "3  it's really aggressive to blast obnoxious ente...   \n",
      "4           and it's a really big bad thing about it   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0                                    [what, said, .]  \n",
      "1  [plus, you, 've, added, commercials, to, the, ...  \n",
      "2  [i, did, n't, today, ..., must, mean, i, need,...  \n",
      "3  [it, 's, really, aggressive, to, blast, obnoxi...  \n",
      "4  [and, it, 's, a, really, big, bad, thing, abou...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK tokenizer (only needed once)\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Apply tokenization to the cleaned text\n",
    "df[\"tokenized_text\"] = df[\"cleaned_text\"].apply(word_tokenize)\n",
    "\n",
    "# Show a preview\n",
    "print(df[[\"cleaned_text\", \"tokenized_text\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0                                         what said.   \n",
      "1  plus you've added commercials to the experienc...   \n",
      "2  i didn't today... must mean i need to take ano...   \n",
      "3  it's really aggressive to blast obnoxious ente...   \n",
      "4           and it's a really big bad thing about it   \n",
      "\n",
      "                                       filtered_text  \n",
      "0                                             said .  \n",
      "1  plus 've added commercials experience ... tacky .  \n",
      "2   n't today ... must mean need take another trip !  \n",
      "3  's really aggressive blast obnoxious entertain...  \n",
      "4                            's really big bad thing  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to tokenize and remove stopwords\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)  # Tokenization\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]  # Stopword removal\n",
    "        return \" \".join(filtered_words)  # Join back into a string\n",
    "    return text\n",
    "\n",
    "# Apply function to the cleaned text column\n",
    "df[\"filtered_text\"] = df[\"cleaned_text\"].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# Show a preview\n",
    "print(df[[\"cleaned_text\", \"filtered_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       filtered_text  \\\n",
      "0                                             said .   \n",
      "1  plus 've added commercials experience ... tacky .   \n",
      "2   n't today ... must mean need take another trip !   \n",
      "3  's really aggressive blast obnoxious entertain...   \n",
      "4                            's really big bad thing   \n",
      "\n",
      "                                        stemmed_text  \n",
      "0                                             said .  \n",
      "1             plu 've ad commerci experi ... tacki .  \n",
      "2     n't today ... must mean need take anoth trip !  \n",
      "3  's realli aggress blast obnoxi entertain guest...  \n",
      "4                            's realli big bad thing  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to apply stemming\n",
    "def apply_stemming(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]  # Apply stemming\n",
    "        return \" \".join(stemmed_words)  # Join words back\n",
    "    return text\n",
    "\n",
    "# Apply function to the cleaned text column\n",
    "df[\"stemmed_text\"] = df[\"filtered_text\"].apply(apply_stemming)\n",
    "\n",
    "# Show a preview\n",
    "print(df[[\"filtered_text\", \"stemmed_text\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00  000  000ft  000lb  0011  0016  0162389030167  0162424965446  \\\n",
      "0  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "1  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "2  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "3  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "4  0.0  0.0    0.0    0.0   0.0   0.0            0.0            0.0   \n",
      "\n",
      "   0162431184663  0167560070877  ...  zigzag  zip  zipper  zombi  zone  zoom  \\\n",
      "0            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "1            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "2            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "3            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "4            0.0            0.0  ...     0.0  0.0     0.0    0.0   0.0   0.0   \n",
      "\n",
      "   zrh  zuke  zurich  zurichnew  \n",
      "0  0.0   0.0     0.0        0.0  \n",
      "1  0.0   0.0     0.0        0.0  \n",
      "2  0.0   0.0     0.0        0.0  \n",
      "3  0.0   0.0     0.0        0.0  \n",
      "4  0.0   0.0     0.0        0.0  \n",
      "\n",
      "[5 rows x 11242 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialize CountVectorizer (Bag of Words)\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(df[\"stemmed_text\"])  # Apply on stemmed text\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"stemmed_text\"])  # Apply on stemmed text\n",
    "\n",
    "# Convert to DataFrames for better readability\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Show first few rows of TF-IDF values\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 11712 samples\n",
      "Testing Set: 2928 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target labels (y)\n",
    "X = df[\"stemmed_text\"]  # Using stemmed text as input\n",
    "y = df[\"airline_sentiment\"]  # Sentiment labels (positive, negative, neutral)\n",
    "\n",
    "# Split the dataset (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show dataset sizes\n",
    "print(f\"Training Set: {len(X_train)} samples\")\n",
    "print(f\"Testing Set: {len(X_test)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.99      0.81      1889\n",
      "     neutral       0.72      0.14      0.24       580\n",
      "    positive       0.90      0.19      0.32       459\n",
      "\n",
      "    accuracy                           0.70      2928\n",
      "   macro avg       0.77      0.44      0.46      2928\n",
      "weighted avg       0.73      0.70      0.62      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Convert text into TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df[\"stemmed_text\"])\n",
    "\n",
    "# Define target labels\n",
    "y = df[\"airline_sentiment\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Naïve Bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
